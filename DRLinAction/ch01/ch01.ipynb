{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <b>Kwang Myung Yu</b></div>\n",
    "<div style=\"text-align: right\"> Initial issue : 2023.06.28 </div>\n",
    "<div style=\"text-align: right\"> last update : 2023.06.28 </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "#plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DP와 MC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 동적 계획법(DP) : 복잡한 고수준의 문제를 작은 문제들로 분해. 작은 문제들은 추가 정보 없이 풀수 있을 정도로 분해\n",
    "\n",
    "DP를 적용하려면 주어진 문제를 (해법을 아는) 부분 문제로 분해해야 한다.   \n",
    "로봇 청소기가 충전 단자까지 이동하기...\n",
    "방을 나간다. -> ..   \n",
    "실제에서는 분해가 불가능 할 수도 있다.   \n",
    "주변 환경을 알지 못하면 어려울 수도 있다.  \n",
    "따라서 주변 환경은 완벽하게 파악하고 목표 지점에 대해서는 아는 것이 없는 상황???  \n",
    "즉 어느정도 모델이 머릿속에 갖추어져야 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC : 환경의 무작이 표집, 시행착오"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 문제에서는 환경을 아예 모리지는 않으므로  DP와 MC를 적절히 섞어서 사용한다.  \n",
    "예를 들어 방을 나가기 위해 조약돌을 던져 보는것??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 강화학습의 틀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 목적 함수\n",
    "- 환경 : 환경은 일종의 동적과정(dynamic process), 즉 시간의 함수임.\n",
    "- 상태(state) : 알고리즘 편의를 위해서 연속 환경 데이터 스트림을 이산적인 조각으로 나누고 묶을 필요가 있다. 이런 개별 데이터 조각을 state라고 한다.\n",
    "- 즉 강화학습 알고리즘은 이산적인 timestep에서 이산적인 state 데이터를 입력 받는다.\n",
    "- reward : 목표를 향해 학습 알고리즘이 알마나 잘 나아가고 있는지 말해주는 (국소적인) 신호. 알고리즘을 갱신하는데 사용할 수 있는 신호는 reward가 유일하다.\n",
    "- 강화학습은 지도학습과 달리 정답이 주어지지 않는다. 예를 들어 온도 제어기에 오차가 감소하면 +10의 reward가 주어지는 형태이다.   \n",
    "- 에이전트 : 강화학습에서 동작을 취하거나 결정을 내리는 모든 학습 알고리즘"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 왜 심층 강화학습인가."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 강화학습은 에이전트의 경험을 참조표, 딕셔너리, 테이블 등에 저장하고 알고리즘을 반복하여 갱신하는 수준이었다.   \n",
    "즉 에이전트가 환경에서 다양한 시도를 하고 그 경험을 데이터베이스에 저장해두고, 나중에 그 데이터베이스를 사람이 분석해서 유익한 시도와 그렇지 않은 시도를 구분하는 방식이었다.   \n",
    "\n",
    "그러나 환경이 복잡해지면 참조표를 소용하는 것이 불가능해진다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝을 사용하면 상태의 중요한 특징들만 추출하능 방법을 학습할 수 있다.   \n",
    "딥러닝 알고리즘의 파라미터는 유한하므로 모든 스테이트를 에이전트가 효율적으로 처리할 수 있는 무언가로 압축하고, 그 압축된 표현으로 에이전트가 결정을 내리도록 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
