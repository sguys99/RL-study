{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 강화학습 문제의 모형화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <b>Kwang Myung Yu</b></div>\n",
    "<div style=\"text-align: right\"> Initial issue : 2023.06.29 </div>\n",
    "<div style=\"text-align: right\"> last update : 2023.06.29 </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "#plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 one-armed bandit, 슬롯머신 문제"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 시행에서 에이전트는 n개의 슬로머신 중 하나를 선택해서 레버를 당긴다.   \n",
    "한 시행에서 에이전트가 할수 있는 총 동작은 n가지이다.   \n",
    "이러한 시행을 k번 반복한다.\n",
    "k번째 시행의 액션 a로 부터 보상 $R_k$f를 받는다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제를 푸는 전략은 처음 몇번동안 서로 다른 레버를 당겨서 리워드를 관찰하는것이다.  \n",
    "그 다음 관찰된 보상이 가장 큰 레버를 계속 당기는 것이다.   \n",
    "\n",
    "이 전략을 위해서 이전 시행들에 기초한 특정 액션의 기대 보상, expected reward라는 개념이 필요하다. k번째 시행에서 액션 a의 기대 보상을 수식으로 $Q_k(a)$라고 표기한다.   \n",
    "즉 기대 보상은 액션의 함수이며, 이 함수는 액션을 주어진 액션을 취했을 때 기대할 수있는 리워드의 양을 돌려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_reward(action, history):\n",
    "    rewards_for_action = history[action]\n",
    "    return sum(rewards_for_action) / len(rewards_for_action)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 즉 k번째 시행에서 액션 a의 기대 보상은 그때까지 a의 모든 리워드의 산술 평균이다.   \n",
    "- 따라서 이전 액션들과 관찰은 이후 액션에 영향을 미친다. 이를 이전 동작들이 현재 동작과 미래 동작들을 강화한다고 말할 수 있다.  \n",
    "- $Q_k(a)$를 뭔가 가치를 말해준다는 점에서 동작 가치함수(action value function)라고 부른다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기대 보상에 근거해서 최선의 동작을 찾는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(actions):\n",
    "    best_action = 0\n",
    "    max_action_value = 0\n",
    "    for i in range(len(actions)):\n",
    "        cur_action_value = get_action_value(actions[i])\n",
    "        if cur_action_value > max_action_value:\n",
    "            best_action = i\n",
    "            max_action_value = cur_action_value\n",
    "            \n",
    "    return best_action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 방식을 사용하면 아직 시도한 적없는 어쩌면 기대 보상이 더 큰 레버를 선택하는 일은 없다. \n",
    "- 이처럼 지금까지의 경험에만 근거해 최선의 것을 선택하는 접근 방식을 greedy method라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
