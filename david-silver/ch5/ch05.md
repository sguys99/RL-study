## Lecture 5. Model-Free Control
---

## 목 차
[1. Introduction]()  
[2. On-Policy Monte_Carlo Control]()  
[3. On-Policy Temporal-Difference Learning]()  
[4. Off-Policy Learning]()

## 1. Introduction
지난 강의에서는 
- Model-free prediction
- MDP를 모를 때 value function을 추정

이번 강의에서는
- Model-free control
- MDP를 모를 때 value function을 최적화

MDP로 모델링할 수 있는 문제들을 살펴보자.
![](20230525080138.png)

이러한 문제들은 대부분의 경우 둘중 하나이다. 
- MDP를 모르지만, experience 샘플링 가능
- MDP는 알지만, 사용하기 너무커서, 샘플링만 가능

이 경우 Model-free control을 사용할 수 있다.

### On and Off-Pollicy Learning
policy는 두가지가 있을 수 있다.  
1) target policy : 최적화하려는 policy
2) behavior policy : env에서 경험을 쌓기 위한 policy

Model-free control에서 사용할 수있는 두가지 학습법이 있다.
- On-policy learning : 1. 2.가 같을 때
  - Learn on the job : 경험을 통해 학습
  - 샘플링된 경험으로부터 policy $\pi$에 대해 학습함

- Off-policy learning : 1. 2가 다를 때
  - Look over someone's shoulder : 다른 사람(에이전트)의 어깨너머로 학습
  - 다른 에이전트의 샘플링된 경험과 폴리시 $\mu$로부터 policy $\pi$에 대해 학습함

### 일반적인 Policy Iteration
3장에서 사용한 일반적인 Policy Iteration(즉, DP)를 살펴보자.
![](20230530081709.png)

$v_\pi$를 추정하기 위한 **Policy evaluation**과, policy를 개선하기 위한 **Policy improvement** 과정을 반복했다.

그렇다면 Model-Free도 MC를 사용해서 Policy iteration을 사용하면 되지 않을까?
불가능하다.   
MDP를 모르면 다음 state가 어떻게 될지 알수 없기 때문에 greedy policy를 만들 수가 없다.   
greedy policy라는 것이, 특정 state에서 이동할 수 있는 가장 좋은 state를 선택하는 것인데, state를 모른다면 불가능하다.  

### Model-Free Policy Iteration Using Action-Value Function
앞의 내용을 정리하면,

- $V(s)$를 사용한 Greedy policy improvement 방식은 MDP 모델이 필요하다. 아래 식을 통해 policy improvement를 진행해야하기 때문이다.
![](20230530082644.png)

- 대신 action value function $Q(s, a)$를 사용한 Greedy policy improvement 방식은 model-free이다. 다음 식을 사용하기 때문이다.

![](20230530082843.png)

즉 action을 취해보고(action의 가지수는 알기 때문에), 리턴의 평균을 취하는 것은 가능하다.


### 일반화된 Policy Iteration with Action-Value Function
이제 **Q**를 사용해서 MC를 통한 평가를 할 수 있음을 알았다.  
![](20230530083224.png)

그렇다면 greedy policy improve는 가능할까?   
greedy 하면 exploration이 충분치 않다.

### Example of Greedy Action Selection
아래 그림의 예에서 처럼 두문중에 하나를 선택하는 문제에서 처음 왼쪽을 선택했을때 reward가 0이 나왔고, 이후 오른쪽 문을 세번 연속 했을때 계속 reward가 나왔다면, 오른쪽 문만 선택하려고 하지 않을까?
![](20230530083415.png)


### $\epsilon$-Greedy Exploration
이러한 문제를 완화하기 이해 제안된 방법이 $\epsilon$-Greedy Exploration 방식이다.  

- non-zero probability로 시도할 수 있는 action이 $m$개 있을 때,
- $1 - \epsilon$의 확률로 greedy action을 취하고,
- \epsilon$의 확률로 greedy action을 취함
- 즉, 다음과 같이 policy를 설정

![](20230530084205.png)

$\epsilon$을 설정할 때 모든 action을 exploration 할 수 있고, 동시에 policy가 개선될 수 있도록 해야한다.

### $\epsilon$-Greedy Policy Improvement
앞의 내용을 정리, 증명한 것이다.
![](20230530084538.png)

### MC Policy Iteration  
이제 Model-free에서 On-policy 기반의 MC policy interation을 정리하면 다음과 같다.  

![](20230530084929.png)
- Policy evaluation : $Q = q_\pi$를 사용한 MC policy evaluation
- Policy improvement : $\epsilon$-greedy 기반

한 에피소드를 수행한 다음에 진행하는 것도 가능하다.(속도 빠름)  
![](20230530085245.png)

### GLIE(Greedy in the Limit with Infinite Exploration)
앞의 MC가 잘 실행되기 위한 조건이 있다.(GLIE)
![](20230530085436.png)
- exploration에 관한것 : 모든 state-action pair가 무한히 많이 exploration 되어야 한다.
- explot에 관한 것 : $\epsilon$ 확률 만큼 랜덤 policy가 있더라도 결국에는 greedy로 수렴해야한다.
  - 예를 들어 $\epsilon$가 $k$ 스텝에서 $\epsilon _k = 1/k$로 되도록 설정하면 GLIE를 만족한다.

### GLIE MC Control
