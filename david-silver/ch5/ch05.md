## Lecture 5. Model-Free Control
---

## 목 차
[1. Introduction]()  
[2. On-Policy Monte_Carlo Control]()  
[3. On-Policy Temporal-Difference Learning]()  
[4. Off-Policy Learning]()

## 1. Introduction
지난 강의에서는 
- Model-free prediction
- MDP를 모를 때 value function을 추정

이번 강의에서는
- Model-free control
- MDP를 모를 때 value function을 최적화

MDP로 모델링할 수 있는 문제들을 살펴보자.
![](20230525080138.png)

이러한 문제들은 대부분의 경우 둘중 하나이다. 
- MDP를 모르지만, experience 샘플링 가능
- MDP는 알지만, 사용하기 너무커서, 샘플링만 가능

이 경우 Model-free control을 사용할 수 있다.

### On and Off-Pollicy Learning
policy는 두가지가 있을 수 있다.  
1) target policy : 최적화하려는 policy
2) behavior policy : env에서 경험을 쌓기 위한 policy

Model-free control에서 사용할 수있는 두가지 학습법이 있다.
- On-policy learning : 1. 2.가 같을 때
  - Learn on the job : 경험을 통해 학습
  - 샘플링된 경험으로부터 policy $\pi$에 대해 학습함

- Off-policy learning : 1. 2가 다를 때
  - Look over someone's shoulder : 다른 사람(에이전트)의 어깨너머로 학습
  - 다른 에이전트의 샘플링된 경험과 폴리시 $\mu$로부터 policy $\pi$에 대해 학습함

### 일반적인 Policy Iteration
3장에서 사용한 일반적인 Policy Iteration(즉, DP)를 살펴보자.
![](20230530081709.png)

$v_\pi$를 추정하기 위한 **Policy evaluation**과, policy를 개선하기 위한 **Policy improvement** 과정을 반복했다.

그렇다면 Model-Free도 MC를 사용해서 Policy iteration을 사용하면 되지 않을까?
불가능하다.   
MDP를 모르면 다음 state가 어떻게 될지 알수 없기 때문에 greedy policy를 만들 수가 없다.   
greedy policy라는 것이, 특정 state에서 이동할 수 있는 가장 좋은 state를 선택하는 것인데, state를 모른다면 불가능하다.  

### Model-Free Policy Iteration Using Action-Value Function
앞의 내용을 정리하면,

- $V(s)$를 사용한 Greedy policy improvement 방식은 MDP 모델이 필요하다. 아래 식을 통해 policy improvement를 진행해야하기 때문이다.
![](20230530082644.png)

- 대신 action value function $Q(s, a)$를 사용한 Greedy policy improvement 방식은 model-free이다. 다음 식을 사용하기 때문이다.

![](20230530082843.png)

즉 action을 취해보고(action의 가지수는 알기 때문에), 리턴의 평균을 취하는 것은 가능하다.


### 일반화된 Policy Iteration with Action-Value Function
이제 **Q**를 사용해서 MC를 통한 평가를 할 수 있음을 알았다.  
![](20230530083224.png)

그렇다면 greedy policy improve는 가능할까?   
greedy 하면 exploration이 충분치 않다.

### Example of Greedy Action Selection
아래 그림의 예에서 처럼 두문중에 하나를 선택하는 문제에서 처음 왼쪽을 선택했을때 reward가 0이 나왔고, 이후 오른쪽 문을 세번 연속 했을때 계속 reward가 나왔다면, 오른쪽 문만 선택하려고 하지 않을까?
![](20230530083415.png)


### $\epsilon$-Greedy Exploration
이러한 문제를 완화하기 이해 제안된 방법이 $\epsilon$-Greedy Exploration 방식이다.  

- non-zero probability로 시도할 수 있는 action이 $m$개 있을 때,
- $1 - \epsilon$의 확률로 greedy action을 취하고,
- \epsilon$의 확률로 greedy action을 취함
- 즉, 다음과 같이 policy를 설정

![](20230530084205.png)

$\epsilon$을 설정할 때 모든 action을 exploration 할 수 있고, 동시에 policy가 개선될 수 있도록 해야한다.

### $\epsilon$-Greedy Policy Improvement
앞의 내용을 정리, 증명한 것이다.
![](20230530084538.png)

### MC Policy Iteration  
이제 Model-free에서 On-policy 기반의 MC policy interation을 정리하면 다음과 같다.  

![](20230530084929.png)
- Policy evaluation : $Q = q_\pi$를 사용한 MC policy evaluation
- Policy improvement : $\epsilon$-greedy 기반

한 에피소드를 수행한 다음에 진행하는 것도 가능하다.(속도 빠름)  
![](20230530085245.png)

### GLIE(Greedy in the Limit with Infinite Exploration)
앞의 MC가 잘 실행되기 위한 조건이 있다.(GLIE)
![](20230530085436.png)
- exploration에 관한것 : 모든 state-action pair가 무한히 많이 exploration 되어야 한다.
- explot에 관한 것 : $\epsilon$ 확률 만큼 랜덤 policy가 있더라도 결국에는 greedy로 수렴해야한다.
  - 예를 들어 $\epsilon$가 $k$ 스텝에서 $\epsilon _k = 1/k$로 되도록 설정하면 GLIE를 만족한다.

### GLIE MC Control
이제 본 강의에서 나오는 첫번째 Model free control 방법에 대해 알아보자.  
바로 GLIE MC Control이다.
![](20230601081857.png)
위에서 $1/N(S_t, A_t)$는 이전의 방식처럼 고정 시킬수도 있다.  
그리고 greedy-policy로 수렴하도록 하기 위해 $\epsilon$을 $1/k$로 설정한다.   

![](20230601082119.png)

### MC Control을 사용함 블랙잭 예제
4장에서는 policy가 설정된 상태에서 각 state의 value를 prediction(evaluation) 해보았다.  
여기서는 최적의 polic를 찾아본다.   

본 문제에서 state는 3개이다.
- ACE 카드 보유여부 : 0, 1
- 딜러가 보여준 카드 (A ~ 10)
- 내카드의 합 (11 ~ 21)

아래는 MC Control로 계산한 $\pi _*$ $v_*$이다.

## 3. On-Policy Temporal-Difference Learning
### MC vs TD Control
그러면 4장에서 처럼 MC 대신 TD를 사용해되 될까?  
가능하다. TD의 장점을 활용할 수 있다.  

- TD learning은 MC와 비교했을 때 몇가지 장점이 있다.  
  - Lower variance
  - Online
  - Incomplete sequences에 사용가능

- 기본 컨셉 : 앞의 제어루프에 TD 대신 MC를 사용
  - $Q(S, A)$를 업데이트 할 때 TD 사용
  - policy improvement를 위해 $\epsilon$-greedy 사용
  - 매 time-step마다 업데이트

### Sarsa($\lambda$)
TD를 사용해서 Action-Value function을 업데이트 하는 방법인 Sarsa에 대해서 살펴보자.
![](20230601084634.png)

- 1. state S에서 action A를 취함
- 2. reward R을 받고 state S`에 도달
- 3. state S\`에서 action A\`를 취하고 이전 스텝 테이블에 있던 $Q(S`, A`)$를 사용

Sarsa도 테이블 업데이트 방식을 사용하고, 업데이트 식은 다음과 같다.
![](20230601085138.png)

$\alpha$ 뒤에 있는 부분은 앞에서 배웠던 TD error이다.


![](20230601085427.png)

정리하면 On-Policy Control With Sarsa는,  
- 매 time-step마다
- Policy evaluation : Sarsa로 $Q \approx q_\pi$를 업데이트 하여
- Policy improvement : $\epsilon$-greedy 사용하여 policy를 improvement한다.

### Sarsa Algorithm for On-Policy Control
Sarsa 알고리즘에 대하여 더 자세히 살펴보자.
