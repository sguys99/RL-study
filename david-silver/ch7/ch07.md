## Lecture 7. Policy Gradient
---

## 목 차
[1. Introduction]()  
[2. Finite Difference Policy Gradient]()  
[3. Monte-Garlo Policy Gradient]()
[4. Actor-Critic Policy Gradient]()

## 1. Introduction
- 앞에서 우리는 Value나 Action value function을 파라미터 $\theta$로 추청하능 방법을 배웠다.
![](20230619083540.png)

- 최적의 policy는 value function으로부터 직접 생성할 수 있었다.
  - 예: $\epsilon$-greedy

- 본 장에서는 **policy**를 직접 파라미터화 한다.
![](20230619083747.png)
- 여기서도 여전히 앞에서와 마찬가지로 model free 문제에 집중한다.

### Value 기반과 Policy 기반 강화학습
Model-Free RL 해결방법은 세가지로 분류할 수 있다.
- Value 기반(6장에서 배움)
  - Value Function을 학습
  - 이를 통해서 Implicit policy(ex: $\epsilon$-greedy를 구함)

- Policy 기반(7장)
  - Value Function 사용안함
  - Policy를 학습

- Actor-Critic(7장) : actor는 policy관련, critic은 value관련
  - Value function, Policy 둘다 학습

![](20230619084506.png)


### Policy 기반 강화학습의 장점
Value 기반 방법이 있는데 Policy 기반 방법을 사용하는 이유는 무엇일까?

- 장점
  - 수렴성이 좋다.
  - 고차원, 또는 continuous action spaces에서 유용하다.(예: 0~1 사이이의 실수 action이 존재한다면)
  - 확률기반(stochastic) policy를 학습가능하다. (6장의 방법은 greedy policy를 선정하는 deterministic 방법이었다.)

- 단점
  - loca optimum에 빠지기 쉽다.
  - policy를 평가하는 방법이 비효율적이고 variance가 크다.
    - value 기반 방법은 매우 공격적인반면 stable하다.(내가 할수 있는 action 중에 max V를 취하는 것을 선택하므로)

### Example
그러면 stochastic policy가 필요한 이유는 무엇일까?  
- 가위가위보 게임
policy가 deterministic이면 상대방에게 알려지면 위험하다.   
결국 uniform random policy가 최적이다.  

![](20230619085311.png)

- Aliased grid world

state가 partially observable한 상황이라고 가정하자.  
예를 들어 회색 부분의 피처가 불완전한 상황....

![](20230619085527.png)

policy가 deterministic하면 회색 지점에 도달 했을떄 빠져나오지 못한다.
![](20230619085628.png)

policy가 stochastic이면 회색 지점을 탈출할 수 있다.
![](20230619085705.png)


### Policy Objective Functions
이제 최적의 policy를 찾기 위한 Policy objective function에 대해서 알아보자.
policy $\pi(s, a)$ state s에서 action a를 확률이다.  
그런데 우리는 파라미터 $\theta$로 구성된 $\pi_\theta(s, a)$ function approxmator를 사용할 것이다.

이제 RL문제의 목표는 다음과 같이 정의된다.
- 목표 : 주어진 policy $\pi_\theta(s, a)$에서 최적의 $\theta$ 찾기
- 그런데 $\pi_\theta$의 좋고 나쁨을 어떻게 측정해야 할까?
- 이를 위해 목적함수 J를 정의할 필요가 있다.
- 대표적인 J는 다음 세가지가 있다.
- 1) episodic 환경에서 우리는 start value를 쓸수 있다. 매번 같은 지점에서 시작하는 것은 아니지만... 따라서 다음과 같이 정의 가능하다.(예를 들어 pc 게임), 시작 state s1는 고정된 값일 필요는 없다. s1이 확률 분포여도 사용가능하다.
![](20230620083015.png)
- 2) continuous 환경에서는 average value를 사용할 수 있다. 즉 해당 state에 위치할 확률 d에 해당 state의 value를 곱한 것을 모두 더한 것이다.
![](20230620083214.png)
- 3) 또는 step 마다 average reward를 사용하여 J를 설정할 수도 있다.
![](20230620083320.png)

여기서 $d^{\pi_\theta}(s)$는 policy $\pi_\theta$에 대한 stationary distribution이다.

### Policy optimization 
이제 J를 정의하는 방법에 대해서 알게되었다. 결국 policy 기반 RL은 최적화 문제이다.  
다시 말해 $J(\theta)$를 최대화 하는 $\theta$를 찾는 문제이다.
해를 찾는 방법은 다양한다. 우리는 gradient descent를 사용할 것이다.

## 2. Finite Difference Policy Gradient
### Policy Gradient
GD 방법은 앞에서 설정한 J를 policy objective function으로 두고 최대 지점을 찾는 것이다.  
![](20230620084227.png)

### Computing Gradients By Finite Differences
최적 지점을 찾기 위한 가장 단순한 방법이 Finite Difference 방법이다.
만약 $\theta$의 차원이 n dimension이라면, 각 차원 마다 $\theta_k$를 조금씩 변경하여 편미분을 구하고 업데이트 하는 방법이다. 이러한 작업을 n번 반복한다.  
이 방법은 단순하지만, 노이즈에 민감하고 비효율적이다.  
그러나 policy가 미분가능하지 않아도 적용가능하다.  

![](20230620084930.png)

그러나 이 방법은 요즘에 잘 사용되지 않고 있다....

## 3. Monte-Carlo Policy Gradient
### Score function
이제 policy gradient를 analytical하게 계산하는 방법을 살펴보자.   
- 먼저 policy $\pi_\theta$가 미분 가능하다고 가정하다.
- 그러면 policy의 gradient $\nabla_\theta \pi_\theta(s, a)$ 를 계산할있다.
- 그러면 likelihood ratio라는 트릭을 사용해서 gradient를 다음과 같이 표현이 가능하다.(로그변환 활용)

![](20230620085925.png)

여기서 $\nabla_\theta log\pi_\theta(s, a)$를 score function이라고 한다.

