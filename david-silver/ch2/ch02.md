## Lecture 2. Markov Decision Processes
---

## 목 차
[1. Markov Processes]()  
[2. Markov Reward Processes]()  
[3. Markov Decision Processes]()  
[4. Extensions to MDPs]()


## 1. Markov Processes
### Introduction
- MDP(Markov Decision Processes)는 강화학습에서 환경(environment)를 묘사함
- MDP에서 환경은 fully observable 함
- 즉, 현재 state는 프로세스를 특성화(characterise)
- 대부분의 강화학습 문제는 MDP로 정의할 수 있다.
  - 최적 제어 문제 -> Continuous MDP
  - Partially observable problem -> MDP로 변환가능
  - 한 개의 state를 가지는 슬롯머신(bandit)

### Markov Property
- 정의 : state $S_t$는 다음 조건을 만족할 때(iif) `Markov`라고 한다.
$$P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t]$$

    - 즉, 미래의 state가 현재 state에만 의존하고, 과거 state에는 독립이다.
    - 따라서 현재 state만 알면 history는 미래 state 예측에 필요하지 않다.

- Markov state $s$, 그리고 이어지는 state $s'$가 있다면 *state transition prob.*는 다음과 같이 정의된다.

$$P_{ss'} = P[S_{t+1} = s' | S_t = s]$$

예를 들어 전체 state가 $n$개 있다면, transition prob.는 행렬 형태로 나타낼 수 있으며, 이를 `state transition mat.`라고 한다. 모든 state $s$에 대해, 이어지는 state $s'$의 transition prob.를 나타낸 것이다.
$$\begin{bmatrix}P_{11}&...&P_{1n}\\ \vdots&&\\P_{n1}&...&P_{nn} \end{bmatrix}$$
    * 여기서 각 row의 합은 1이다.

### Markov Process
- 정의 : Markov Process는 $<S, P>$로 표현된다. 여기서
  - $S$는 유한한 개수의 state 집합(set)
  - $P$는 state transition matrix
    - $P_{ss'} = P[S_{t+1} = s' | S_t = s]$

- Markov Process는 `memoryless random process`이다. 
- 즉, Markov Property를 가지는 랜덤 state의 시퀀스이다. ($S_1, S_2, ...$)

- Example : Student markov chain

![](2023-04-23-20-36-03.png)

- 에피소드 : 특정 state에서 시작해서 터미널 state까지의 시퀀스
  - MP는 여러 종류의 에피소드를 샘플링 가능

![](2023-04-23-20-40-55.png)

각 state간 transition prob.는 행렬로 표현 가능

![](2023-04-23-20-43-27.png)


## 2. Markov Reward Process
### Markov Reward Process
- 정의 : Markov Reward Process는 $<S, P, R, \gamma>$로 표현된다. 여기서
  - $S$는 유한한 개수의 state 집합(set)
  - $P$는 state transition matrix, $P_{ss'} = P[S_{t+1} = s' | S_t = s]$
  - $R$은 reward function, $R_s = E[R_{t+1} | S_t = s]$
    - $R_s$는 state $s$에서 받게되는 리워드($R_{t+1}$ 노테이션에서 알 수 있듯이, state를 떠날때 리워드를 받는다.)
  - $\gamma$는 discount factor, $\gamma \in [0, 1]$

- Example : Student MRP
  - MP와 비교했을 때 state에 리워드가 추가된 형태

![](2023-04-23-20-54-42.png)

### Return
- 정의 : return $G_t$는 time-step $t$에서 경로 구간 동안 받게되는 discounted reward의 총합이다.

$$G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$$
- discount $\gamma$로 미래의 reward의 현재 가치를 나타낸다.
- $\gamma$가 0에 가까우면 근시안적(myopic), 1에 가까우면 멀리봄

- discount가 있으면 
  - 1보다 작은 값으로 설정하면 리턴이 수렴하므로 수학적인 계산이 편하다.
  - 모든 시퀀스의 termination이 보장되면 1로 설정해도 된다.

### Value Function
- 정의 : MRP에서 state value function $v(s)$는 state $s$에서의 return의 기대값이다.

$$v(s) = E[G_t | S_t = s]$$


- Example : Student MRP에서 return
  - 특정 state $S_1$에서($S_1 = C_1)$ 시작에서 terminal state(Sleep)에 도달하기까지의 다양한 에피소드를 샘플링한 후 각각의 return을 계산할 수 있다.
  - 모든 에피소드의 return을 구하여 평균을 취하면 그 결과가 $v(C_1)$이 된다.

![](20230427083141.png)
  - 아래 다이어그램처럼 특정 state에 도달했을 때 state-value function $v(s)$를 표현할 수 있다.
    - 바로 아래 그림에서는 $\gamma=0$이어서 $v(s) = R_s$가 되었다. 
![](20230427083638.png)

![](20230427084029.png)

### Bellman Equation for MRP
- State value function을 어떻게 구할 수 있을까?
- 앞에서 각 state별 에피소드를 많이 샘플링해서 구해진 return의 평균을 취하면 구할 수 있다고 했다.
- 하지만 state 별 구해야할 샘플이 매우 많으면 계산이 어렵다.
- 여기서는 $v(s)$를 구하기 위한 Bellman Equation에 대해 설명한다.

- $v(s)$는 두개 파트로 나누어 질 수 있다.
  - 특정 state에 도달 했을 때 즉각적으로 받는 reward $R_{t+1}$
  - 그 다음 state부터 받게되는 discounted value $\gamma v(S_{t+1})$

![](20230427085012.png)
- 위 $v(s)$를 재정리 한 식에 따르면 v(s)를 구하기 위해 현재 state의 리워드와 이후 state의 $v$를 알 수있으면 된다.

