## Lecture 4. Model-Free Prediction
---

## 목 차
[1. Introduction]()  
[2. Monte-Carlo Learning]()  
[3. Temporal-Difference Learning]()  
[4. TD($\lambda$)]()

## 1. Introduction
지금까지 공부한 내용을 정리해보자.
- ch2 : MDP 설명
- ch3 : MDP를 알고 있을 때 다음 문제를 Dynamic Programming으로 푸는 방법
  - prediction : value evaluation
  - control : policy improvement

본장에서는 **MDP를 모를 때 prediction 해결 방법**에 대해 배운다.  
다음장에서는 **MDP를 모을때 control 해결 방법**에 대해 배운다.

![](20230510085401.png)

Model-Free prediction 방법에는 다음 두가지가 있다.
- Monte-Carlo
- Temporal-Differnce

## 2. Monte-Carlo Learning

Model-Free prediction에서는 policy가 정해져 있다. 따라서 MC 방법은 Emperical하게 action을 가해보고, 취득된 리턴을 구해서 평균을 취하는 방법이다.
- 직접 에피소드 경험을 통해 확습한다.
- Model-Free : MDP의 transition이나 리워드에 대한 지식이 필요없다.
- 에피소드가 끝난 후 학습한다.(?) : 예측 값을 이용해 또 다른 값을 예측하는 부트스트래핑 방식이 아니다.(?) 
  - 참고: 부트스트래핑을 사용하면 bias는 커지고 variance는 낮아짐
- `value = 평균 return`이라는 간단한 아이디어를 사용 
- 주의 : epsodic(에피소드가 종료되는) MDP에서만 MC를 사용할 수있다.
  - All episodes must terminate

### MC 를 사용한 Policy Evaluation(Prediction)
- 목표: 주어진 policy $\pi$ 에 따라 에피소드를 실행하여 취득한 $v_{\pi}를 학습. 예를 들어 state S에서 액션 A를 취했을 때 리턴 R을 모두 취득하고...
![](20230511080218.png)

- 리턴 계산식은 다음과 같고...
![](20230511080311.png)

- value function은 return의 기대값으로 다음과 같으니까...
 ![](20230511080413.png)

 - MC policy evaluation 방법은 **예측 값**이 아니로 경험으로 구한 평균(**emperical mean**)을 사용한다.
 - 대표적인 MC 방법에는 다음 두가지가 있다.  
   - First-Visit 
   - Every-Visit 

### First-Visit MC Policy Evaluation






