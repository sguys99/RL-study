## Lecture 4. Model-Free Prediction
---

## 목 차
[1. Introduction]()  
[2. Monte-Carlo Learning]()  
[3. Temporal-Difference Learning]()  
[4. TD($\lambda$)]()

## 1. Introduction
지금까지 공부한 내용을 정리해보자.
- ch2 : MDP 설명
- ch3 : MDP를 알고 있을 때 다음 문제를 Dynamic Programming으로 푸는 방법
  - prediction : value evaluation
  - control : policy improvement

본장에서는 **MDP를 모를 때 prediction 해결 방법**에 대해 배운다.  
다음장에서는 **MDP를 모을때 control 해결 방법**에 대해 배운다.

![](20230510085401.png)

Model-Free prediction 방법에는 다음 두가지가 있다.
- Monte-Carlo
- Temporal-Differnce

## 2. Monte-Carlo Learning

Model-Free prediction에서는 policy가 정해져 있다. 따라서 MC 방법은 Emperical하게 action을 가해보고, 취득된 리턴을 구해서 평균을 취하는 방법이다.
- 직접 에피소드 경험을 통해 확습한다.
- Model-Free : MDP의 transition이나 리워드에 대한 지식이 필요없다.
- 에피소드가 끝난 후 학습한다.(?) : 예측 값을 이용해 또 다른 값을 예측하는 부트스트래핑 방식이 아니다.(?) 
  - 참고: 부트스트래핑을 사용하면 bias는 커지고 variance는 낮아짐
- `value = 평균 return`이라는 간단한 아이디어를 사용 
- 주의 : epsodic(에피소드가 종료되는) MDP에서만 MC를 사용할 수있다.
  - All episodes must terminate

### MC 를 사용한 Policy Evaluation(Prediction)
- 목표: 주어진 policy $\pi$ 에 따라 에피소드를 실행하여 취득한 $v_{\pi}를 학습. 예를 들어 state S에서 액션 A를 취했을 때 리턴 R을 모두 취득하고...
![](20230511080218.png)

- 리턴 계산식은 다음과 같고...
![](20230511080311.png)

- value function은 return의 기대값으로 다음과 같으니까...
 ![](20230511080413.png)

 - MC policy evaluation 방법은 **예측 값**이 아니라 경험으로 구한 평균(**emperical mean**)을 사용한다.
 - 대표적인 MC 방법에는 다음 두가지가 있다.  
   - First-Visit 
   - Every-Visit 

### First-Visit MC Policy Evaluation
$v_\pi(s)$를 업데이트 할 때 처음 $s$를 방문한 케이스만 고려하는 방법이다.
예를 들어
- 에피소드를 진행하면서 state $s$를 **처음** 방문한 시점에
- 방문 카운터를 증가 시켜줌 $N(s) \leftarrow N(s)$  
- 확인된 리턴을 더해줌 $S(s) \leftarrow S(s) + G_t$
- 평균을 취해서 리턴을 계산 $V(s) = S(s) / N(s)$
- $N(s)$가 무한으로 가면 $V(s)$는 $v_\pi (s)$에 수렴한다.
  - $V(s) \rightarrow v_\pi(s)$ as $N(s) \rightarrow \infin$

### Every-Visiti MC Policy Evaluation
반면 Every-visit 방법은 에피소트 안에서 state $s$를 여러번 방문하면 모두 고려한다.
예를 들어
- 에피소드를 진행하면서 state $s$를 방문할 때마다(Every time-step)
- 방문 카운터를 증가 시켜줌 $N(s) \leftarrow N(s)$  
- 확인된 리턴을 더해줌 $S(s) \leftarrow S(s) + G_t$
- 평균을 취해서 리턴을 계산 $V(s) = S(s) / N(s)$
- $N(s)$가 무한으로 가면 $V(s)$는 $v_\pi (s)$에 수렴한다.
  - $V(s) \rightarrow v_\pi(s)$ as $N(s) \rightarrow \infin$


참고 : First-visit, Every-visit 어떤 것을 써도 결과는 유사하다.??

### Blackjack Example
- state 수 : 200
  - 현재 합계 : 12 ~ 21
  - 딜러가 보여주는 카드 : ace ~ 10
  - ace 보유 여부 : yes or no

- action
  - stick : 카드받기 멈춤 (그리고 에피소드 종료)
  - twist : 카드받기

- reward  
  - stick
    - 카드 숫자 합계 > 딜러 카드 숫자 합계 : +1
    - 카드 숫자 합계 = 딜러 카드 숫자 합계 : 0
    - 카드 숫자 합계 < 딜러 카드 숫자 합계 : -1

  - twist
    - 카드 숫자 합계 > 21 : -1 (그리고 에피소드 종료)
    - 그외에 0

- transition : 카드 숫자 합계가 12보다 작으면 자동으로 twist

다음과 같은 policy로 에피소드를 진행하고 value를 계산하였다.
- policy
  - stick : 카드 숫자 합계가 20이상 일때
  - twist : 그외

약 500,000번 에피소드를 진행하고 MC 기반으로 value를 평가했다.
![](20230518085229.png)


### Incremental Mean
MC에서는 매 step마다 방문한 state의 리턴을 확인한 후 평균을 취한다. 그런데 평균을 아래와 같이 표현할 수 있다.

![](20230518085458.png)

즉, 이전 스텝의 결과에 최근 계산한 오차만큼 업데이트 시키는 형태로 표현할 수있다.

이것을 MC 업데이트에 반영하여 표현하면 아래와 같다.

- 에피소드($S_1, A_1, R_2, ..., S_T$) 종료후에 $V(s)$ 업데이트를 하는데
- 리턴 $G_t$로 확인된 각 state $S_t$에 대해서 다음과 같이 업데이트한다.

![](20230518184656.png)

위에서 $(G_t - V(S_t))$가 에러 텀이다.  
그런데 스텝이 증가할수록 $ㅜ$








